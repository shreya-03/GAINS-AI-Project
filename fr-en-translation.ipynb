{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import codecs\n",
    "#import ic\n",
    "import logging\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_sentences = []\n",
    "with codecs.open('europarl-v7.fr-en.en', encoding='utf-8') as f:\n",
    "    english_sentences = f.readlines()\n",
    "english_sentences = [x.strip() for x in english_sentences]\n",
    "f.close()\n",
    "\n",
    "french_sentences = []\n",
    "with codecs.open('europarl-v7.fr-en.fr',encoding='utf-8') as f:\n",
    "    french_sentences = f.readlines()\n",
    "french_sentences = [x.strip() for x in french_sentences]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist( sentence, remove_stopwords=False ):\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\",\" \",sentence)\n",
    "    words = text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to parse text to sentences using tokenizer mentioned above\n",
    "def text_to_sentences(text,tokenizer,remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append( sentence_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk,re\n",
    "tokenizer_fr = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "tokenizer_en = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n"
     ]
    }
   ],
   "source": [
    "documents_en = []\n",
    "documents_fr = []\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer\n",
    "for idx, text in enumerate(english_sentences):\n",
    "    if idx%100 == 0:\n",
    "        print(idx)\n",
    "    x = [word_tokenize(text)]\n",
    "    documents_en += x\n",
    "\n",
    "w = WhitespaceTokenizer()\n",
    "for idx,text in enumerate(french_sentences):\n",
    "    if idx%100 == 0:\n",
    "        print idx\n",
    "    x = [w.tokenize(text)]\n",
    "    documents_fr += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Resumption', u'of', u'the', u'session'], [u'I', u'declare', u'resumed', u'the', u'session', u'of', u'the', u'European', u'Parliament', u'adjourned', u'on', u'Friday', u'17', u'December', u'1999', u',', u'and', u'I', u'would', u'like', u'once', u'again', u'to', u'wish', u'you', u'a', u'happy', u'new', u'year', u'in', u'the', u'hope', u'that', u'you', u'enjoyed', u'a', u'pleasant', u'festive', u'period', u'.'], [u'Although', u',', u'as', u'you', u'will', u'have', u'seen', u',', u'the', u'dreaded', u\"'millennium\", u'bug', u\"'\", u'failed', u'to', u'materialise', u',', u'still', u'the', u'people', u'in', u'a', u'number', u'of', u'countries', u'suffered', u'a', u'series', u'of', u'natural', u'disasters', u'that', u'truly', u'were', u'dreadful', u'.'], [u'You', u'have', u'requested', u'a', u'debate', u'on', u'this', u'subject', u'in', u'the', u'course', u'of', u'the', u'next', u'few', u'days', u',', u'during', u'this', u'part-session', u'.'], [u'In', u'the', u'meantime', u',', u'I', u'should', u'like', u'to', u'observe', u'a', u'minute', u\"'\", u's', u'silence', u',', u'as', u'a', u'number', u'of', u'Members', u'have', u'requested', u',', u'on', u'behalf', u'of', u'all', u'the', u'victims', u'concerned', u',', u'particularly', u'those', u'of', u'the', u'terrible', u'storms', u',', u'in', u'the', u'various', u'countries', u'of', u'the', u'European', u'Union', u'.'], [u'Please', u'rise', u',', u'then', u',', u'for', u'this', u'minute', u\"'\", u's', u'silence', u'.'], [u'(', u'The', u'House', u'rose', u'and', u'observed', u'a', u'minute', u\"'\", u's', u'silence', u')'], [u'Madam', u'President', u',', u'on', u'a', u'point', u'of', u'order', u'.'], [u'You', u'will', u'be', u'aware', u'from', u'the', u'press', u'and', u'television', u'that', u'there', u'have', u'been', u'a', u'number', u'of', u'bomb', u'explosions', u'and', u'killings', u'in', u'Sri', u'Lanka', u'.'], [u'One', u'of', u'the', u'people', u'assassinated', u'very', u'recently', u'in', u'Sri', u'Lanka', u'was', u'Mr', u'Kumar', u'Ponnambalam', u',', u'who', u'had', u'visited', u'the', u'European', u'Parliament', u'just', u'a', u'few', u'months', u'ago', u'.']]\n"
     ]
    }
   ],
   "source": [
    "print documents_en[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-09 14:21:34,638 : INFO : collecting all words and their counts\n",
      "2017-12-09 14:21:34,639 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-09 14:21:34,686 : INFO : collected 11556 word types from a corpus of 79452 raw words and 3000 sentences\n",
      "2017-12-09 14:21:34,688 : INFO : Loading a fresh vocabulary\n",
      "2017-12-09 14:21:34,711 : INFO : min_count=5 retains 1974 unique words (17% of original 11556, drops 9582)\n",
      "2017-12-09 14:21:34,712 : INFO : min_count=5 leaves 64955 word corpus (81% of original 79452, drops 14497)\n",
      "2017-12-09 14:21:34,726 : INFO : deleting the raw counts dictionary of 11556 items\n",
      "2017-12-09 14:21:34,727 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2017-12-09 14:21:34,729 : INFO : downsampling leaves estimated 44782 word corpus (68.9% of prior 64955)\n",
      "2017-12-09 14:21:34,730 : INFO : estimated required memory for 1974 words and 300 dimensions: 5724600 bytes\n",
      "2017-12-09 14:21:34,743 : INFO : resetting layer weights\n",
      "2017-12-09 14:21:34,787 : INFO : training model with 36 workers on 1974 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-12-09 14:21:34,789 : INFO : expecting 3000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-12-09 14:21:35,114 : INFO : worker thread finished; awaiting finish of 35 more threads\n",
      "2017-12-09 14:21:35,116 : INFO : worker thread finished; awaiting finish of 34 more threads\n",
      "2017-12-09 14:21:35,144 : INFO : worker thread finished; awaiting finish of 33 more threads\n",
      "2017-12-09 14:21:35,196 : INFO : worker thread finished; awaiting finish of 32 more threads\n",
      "2017-12-09 14:21:35,217 : INFO : worker thread finished; awaiting finish of 31 more threads\n",
      "2017-12-09 14:21:35,221 : INFO : worker thread finished; awaiting finish of 30 more threads\n",
      "2017-12-09 14:21:35,225 : INFO : worker thread finished; awaiting finish of 29 more threads\n",
      "2017-12-09 14:21:35,227 : INFO : worker thread finished; awaiting finish of 28 more threads\n",
      "2017-12-09 14:21:35,231 : INFO : worker thread finished; awaiting finish of 27 more threads\n",
      "2017-12-09 14:21:35,234 : INFO : worker thread finished; awaiting finish of 26 more threads\n",
      "2017-12-09 14:21:35,252 : INFO : worker thread finished; awaiting finish of 25 more threads\n",
      "2017-12-09 14:21:35,255 : INFO : worker thread finished; awaiting finish of 24 more threads\n",
      "2017-12-09 14:21:35,258 : INFO : worker thread finished; awaiting finish of 23 more threads\n",
      "2017-12-09 14:21:35,262 : INFO : worker thread finished; awaiting finish of 22 more threads\n",
      "2017-12-09 14:21:35,266 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2017-12-09 14:21:35,274 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2017-12-09 14:21:35,278 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-12-09 14:21:35,288 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-12-09 14:21:35,290 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-12-09 14:21:35,291 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-12-09 14:21:35,293 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-12-09 14:21:35,294 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-12-09 14:21:35,295 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-12-09 14:21:35,297 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-12-09 14:21:35,298 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-12-09 14:21:35,299 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-12-09 14:21:35,300 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-12-09 14:21:35,301 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-12-09 14:21:35,303 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-12-09 14:21:35,304 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-09 14:21:35,305 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-09 14:21:35,306 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-09 14:21:35,308 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-09 14:21:35,309 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-09 14:21:35,310 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-09 14:21:35,312 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-09 14:21:35,313 : INFO : training on 397260 raw words (223543 effective words) took 0.5s, 449948 effective words/s\n",
      "2017-12-09 14:21:35,314 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-09 14:21:35,320 : INFO : collecting all words and their counts\n",
      "2017-12-09 14:21:35,323 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-12-09 14:21:35,369 : INFO : collected 6662 word types from a corpus of 85430 raw words and 3000 sentences\n",
      "2017-12-09 14:21:35,371 : INFO : Loading a fresh vocabulary\n",
      "2017-12-09 14:21:35,383 : INFO : min_count=5 retains 1832 unique words (27% of original 6662, drops 4830)\n",
      "2017-12-09 14:21:35,385 : INFO : min_count=5 leaves 77361 word corpus (90% of original 85430, drops 8069)\n",
      "2017-12-09 14:21:35,395 : INFO : deleting the raw counts dictionary of 6662 items\n",
      "2017-12-09 14:21:35,398 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-12-09 14:21:35,400 : INFO : downsampling leaves estimated 50668 word corpus (65.5% of prior 77361)\n",
      "2017-12-09 14:21:35,402 : INFO : estimated required memory for 1832 words and 300 dimensions: 5312800 bytes\n",
      "2017-12-09 14:21:35,412 : INFO : resetting layer weights\n",
      "2017-12-09 14:21:35,450 : INFO : training model with 36 workers on 1832 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-12-09 14:21:35,451 : INFO : expecting 3000 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-12-09 14:21:35,898 : INFO : worker thread finished; awaiting finish of 35 more threads\n",
      "2017-12-09 14:21:35,907 : INFO : worker thread finished; awaiting finish of 34 more threads\n",
      "2017-12-09 14:21:35,914 : INFO : worker thread finished; awaiting finish of 33 more threads\n",
      "2017-12-09 14:21:35,919 : INFO : worker thread finished; awaiting finish of 32 more threads\n",
      "2017-12-09 14:21:35,947 : INFO : worker thread finished; awaiting finish of 31 more threads\n",
      "2017-12-09 14:21:35,950 : INFO : worker thread finished; awaiting finish of 30 more threads\n",
      "2017-12-09 14:21:35,952 : INFO : worker thread finished; awaiting finish of 29 more threads\n",
      "2017-12-09 14:21:35,962 : INFO : worker thread finished; awaiting finish of 28 more threads\n",
      "2017-12-09 14:21:35,969 : INFO : worker thread finished; awaiting finish of 27 more threads\n",
      "2017-12-09 14:21:35,976 : INFO : worker thread finished; awaiting finish of 26 more threads\n",
      "2017-12-09 14:21:35,982 : INFO : worker thread finished; awaiting finish of 25 more threads\n",
      "2017-12-09 14:21:35,986 : INFO : worker thread finished; awaiting finish of 24 more threads\n",
      "2017-12-09 14:21:35,989 : INFO : worker thread finished; awaiting finish of 23 more threads\n",
      "2017-12-09 14:21:35,991 : INFO : worker thread finished; awaiting finish of 22 more threads\n",
      "2017-12-09 14:21:35,992 : INFO : worker thread finished; awaiting finish of 21 more threads\n",
      "2017-12-09 14:21:35,994 : INFO : worker thread finished; awaiting finish of 20 more threads\n",
      "2017-12-09 14:21:35,995 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2017-12-09 14:21:35,996 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2017-12-09 14:21:35,998 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2017-12-09 14:21:35,999 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2017-12-09 14:21:36,000 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2017-12-09 14:21:36,003 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2017-12-09 14:21:36,004 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2017-12-09 14:21:36,005 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2017-12-09 14:21:36,007 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2017-12-09 14:21:36,008 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2017-12-09 14:21:36,010 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2017-12-09 14:21:36,012 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2017-12-09 14:21:36,013 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2017-12-09 14:21:36,017 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-12-09 14:21:36,020 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-12-09 14:21:36,022 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-12-09 14:21:36,026 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-12-09 14:21:36,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-12-09 14:21:36,028 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-12-09 14:21:36,030 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-12-09 14:21:36,031 : INFO : training on 427150 raw words (253455 effective words) took 0.6s, 449480 effective words/s\n",
      "2017-12-09 14:21:36,033 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2017-12-09 14:21:36,040 : INFO : saving Word2Vec object under modelfr, separately None\n",
      "2017-12-09 14:21:36,046 : INFO : not storing attribute syn0norm\n",
      "2017-12-09 14:21:36,047 : INFO : not storing attribute cum_table\n",
      "2017-12-09 14:21:36,085 : INFO : saved modelfr\n",
      "2017-12-09 14:21:36,086 : INFO : saving Word2Vec object under modelen, separately None\n",
      "2017-12-09 14:21:36,087 : INFO : not storing attribute syn0norm\n",
      "2017-12-09 14:21:36,088 : INFO : not storing attribute cum_table\n",
      "2017-12-09 14:21:36,117 : INFO : saved modelen\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "model_fr_wiki = gensim.models.Word2Vec(documents_fr,size=300, window=10, min_count=5, workers=36)\n",
    "model_en_wiki = gensim.models.Word2Vec(documents_en,size=300, window=10, min_count=5, workers=36)\n",
    "\n",
    "model_fr_wiki.save('modelfr')\n",
    "model_en_wiki.save('modelen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-09 14:21:37,533 : INFO : loading Word2Vec object from modelfr\n",
      "2017-12-09 14:21:37,553 : INFO : loading wv recursively from modelfr.wv.* with mmap=None\n",
      "2017-12-09 14:21:37,555 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-12-09 14:21:37,556 : INFO : setting ignored attribute cum_table to None\n",
      "2017-12-09 14:21:37,557 : INFO : loaded modelfr\n",
      "2017-12-09 14:21:37,563 : INFO : loading Word2Vec object from modelen\n",
      "2017-12-09 14:21:37,578 : INFO : loading wv recursively from modelen.wv.* with mmap=None\n",
      "2017-12-09 14:21:37,580 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-12-09 14:21:37,583 : INFO : setting ignored attribute cum_table to None\n",
      "2017-12-09 14:21:37,584 : INFO : loaded modelen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    }
   ],
   "source": [
    "print 'Loading models...'\n",
    "model_source = gensim.models.Word2Vec.load('modelfr')\n",
    "model_target = gensim.models.Word2Vec.load('modelen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training pairs...\n"
     ]
    }
   ],
   "source": [
    "print 'Reading training pairs...'\n",
    "word_pairs = codecs.open('topfrenwords.csv', 'r', 'utf-8')\n",
    "pairs = pd.read_csv(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id         english           francais\n",
      "0        0             the                 la\n",
      "1        1              of                 de\n",
      "2        2              to                  à\n",
      "3        3             and                 et\n",
      "4        4               a                une\n",
      "5        5              in                 en\n",
      "6        6             for               pour\n",
      "7        7              is                est\n",
      "8        8             the                 le\n",
      "9        9            that                que\n",
      "10      10              on                sur\n",
      "11      11            said                dit\n",
      "12      12            with               avec\n",
      "13      13              be               être\n",
      "14      14             was              était\n",
      "15      15              by                par\n",
      "16      16              as              comme\n",
      "17      17             are               sont\n",
      "18      18              at                  à\n",
      "19      19              it                 il\n",
      "20      20             has                  a\n",
      "21      21              an                une\n",
      "22      22            have              avoir\n",
      "23      23            will            volonté\n",
      "24      24              or                 ou\n",
      "25      25             its                son\n",
      "26      26              he                 il\n",
      "27      27             not                pas\n",
      "28      28            were            étaient\n",
      "29      29           which                qui\n",
      "...    ...             ...                ...\n",
      "9145  9145          tribal             tribal\n",
      "9146  9146        delicate            délicat\n",
      "9147  9147       fragments             débris\n",
      "9148  9148      repurchase             rachat\n",
      "9149  9149         furnace               four\n",
      "9150  9150         broaden            élargir\n",
      "9151  9151          mecham             mecham\n",
      "9152  9152       conductor         conducteur\n",
      "9153  9153      expressing           exprimer\n",
      "9154  9154      washington         washington\n",
      "9155  9155         hyundai            hyundai\n",
      "9156  9156        relieved            soulagé\n",
      "9157  9157       belonging       appartenance\n",
      "9158  9158          valley             vallée\n",
      "9159  9159         prefers            préfère\n",
      "9160  9160            visa               visa\n",
      "9161  9161        definite             précis\n",
      "9162  9162         fragile            fragile\n",
      "9163  9163         rewards        récompenses\n",
      "9164  9164        respects           respects\n",
      "9165  9165         careers          carrières\n",
      "9166  9166           seize             saisir\n",
      "9167  9167     inefficient         inefficace\n",
      "9168  9168      conceptual         conceptuel\n",
      "9169  9169       densities           densités\n",
      "9170  9170             eps                eps\n",
      "9171  9171              me                moi\n",
      "9172  9172           sparc              sparc\n",
      "9173  9173         spirits         spiritueux\n",
      "9174  9174  experimentally  expérimentalement\n",
      "\n",
      "[9175 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1974\n"
     ]
    }
   ],
   "source": [
    "print len(model_source.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of missing vocab:  8259\n"
     ]
    }
   ],
   "source": [
    "missing = 0\n",
    "\n",
    "for n in range (len(pairs)):\n",
    "#     print pairs['francais']\n",
    "    if pairs['francais'][n] not in model_source.wv.vocab or pairs['english'][n] not in model_target.wv.vocab:\n",
    "        missing = missing + 1\n",
    "        pairs = pairs.drop(n)\n",
    "pairs = pairs.reset_index(drop = True)\n",
    "print 'Amount of missing vocab: ', missing\n",
    "\n",
    "# make list of pair words, excluding the missing vocabs \n",
    "# removed in previous step\n",
    "pairs['vector_french'] = [model_source[pairs['francais'][n]] for n in range (len(pairs))]\n",
    "pairs['vector_english'] = [model_target[pairs['english'][n]] for n in range (len(pairs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(916, 5)\n"
     ]
    }
   ],
   "source": [
    "print pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first 5000 from both languages, to train translation matrix\n",
    "source_training_set = pairs['vector_french'][:800]\n",
    "target_training_set = pairs['vector_english'][:800]\n",
    "\n",
    "matrix_train_source = pd.DataFrame(source_training_set.tolist()).values\n",
    "matrix_train_target = pd.DataFrame(target_training_set.tolist()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating translation matrix\n"
     ]
    }
   ],
   "source": [
    "print 'Generating translation matrix'\n",
    "\n",
    "# Matrix W is given in  http://stackoverflow.com/questions/27980159/fit-a-linear-transformation-in-python\n",
    "translation_matrix = np.linalg.pinv(matrix_train_source).dot(matrix_train_target).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar_vector(self, vectenter, topn=5):\n",
    "    self.init_sims()\n",
    "    dists = np.dot(self.wv.syn0norm, vectenter)\n",
    "    if not topn:\n",
    "        return dists\n",
    "    best = np.argsort(dists)[::-1][:topn ]\n",
    "    # ignore (don't return) words from the input\n",
    "    result = [(self.wv.index2word[sim], float(dists[sim])) for sim in best]\n",
    "    return result[:topn]\n",
    "\n",
    "def top_translations(w,numb=5):\n",
    "    val = most_similar_vector(model_target,translation_matrix.dot(model_source[w]),numb)\n",
    "    #print 'traducwithscofres ', val\n",
    "    return val\n",
    "\n",
    "\n",
    "def top_translations_list(w, numb=1):\n",
    "    val = [top_translations(w,numb)[k][0] for k in range(numb)]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = 1\n",
    "#top_matches = [ pairs['target'][n] in top_translations_list(pairs['source'][n]) for n in range(5000,5003)] \n",
    "\n",
    "# print out source word and translation\n",
    "def display_translations():\n",
    "    for word_num in range(range_start, range_end):\n",
    "        source_word =  pairs['francais'][word_num]\n",
    "        translations = top_translations_list(pairs['francais'][word_num]) \n",
    "        print source_word, translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-09 14:22:51,879 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy @5 is  13 / 115\n",
      "Accuracy @1 is  13 / 115\n"
     ]
    }
   ],
   "source": [
    "# range to use to check accuracy\n",
    "range_start = 801\n",
    "range_end = 916\n",
    "\n",
    "#print (pairs['english'])\n",
    "#display_translations()\n",
    "\n",
    "# now we can check for accuracy on words 5000-6000, 1-5000 used to traning\n",
    "# translation matrix\n",
    "\n",
    "# returns matrix of true or false, true if translation is accuracy, false if not\n",
    "# accurate means the first translation (most similiar vector in target language)\n",
    "# is identical\n",
    "accuracy_at_five = [pairs['english'][n] in top_translations_list(pairs['francais'][n]) for n in range(range_start, range_end)]\n",
    "print 'Accuracy @5 is ', sum(accuracy_at_five), '/', len(accuracy_at_five)\n",
    "\n",
    "accuracy_at_one = [pairs['english'][n] in top_translations_list(pairs['francais'][n],1) for n in range(range_start, range_end)]\n",
    "print 'Accuracy @1 is ', sum(accuracy_at_one), '/', len(accuracy_at_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amendment']\n",
      "amendment\n"
     ]
    }
   ],
   "source": [
    "print top_translations_list(pairs['francais'][801])\n",
    "print pairs['english'][801]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amendment', 'NN')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tag.pos_tag(['amendment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokens_count(tokens):\n",
    "    dict_tokens = {}\n",
    "    #for sentence in content:\n",
    "    for token in tokens:\n",
    "        if token not in dict_tokens:\n",
    "            dict_tokens[token] = 1\n",
    "        else:\n",
    "            dict_tokens[token] += 1\n",
    "    return dict_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigrams(input_list):\n",
    "    bigram_dict = {}\n",
    "    for i in range(len(input_list)-1):\n",
    "        if (input_list[i],input_list[i+1]) not in bigram_dict:\n",
    "            bigram_dict[(input_list[i],input_list[i+1])] = 1\n",
    "        else:\n",
    "            bigram_dict[(input_list[i],input_list[i+1])] += 1\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trigrams(input_list):\n",
    "    trigrams_dict = {}\n",
    "    for i in range(len(input_list)-2):\n",
    "        if (input_list[i],input_list[i+1],input_list[i+2]) not in trigrams_dict:\n",
    "            trigrams_dict[(input_list[i],input_list[i+1],input_list[i+2])] = 1\n",
    "        else:\n",
    "            trigrams_dict[(input_list[i],input_list[i+1],input_list[i+2])] += 1\n",
    "    return trigrams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for text in english_sentences:\n",
    "    tokens += word_tokenize(text)\n",
    "#print tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigrams = tokens_count(tokens)\n",
    "bigrams = get_bigrams(tokens)\n",
    "trigrams = get_trigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KneserNey:\n",
    "    \n",
    "    def __init__(self,text,n):\n",
    "        self.text = text\n",
    "        self.n = n\n",
    "        self.ngram_counts = {}\n",
    "        self.lowergram_counts = {}\n",
    "        self.input_list = []\n",
    "        self.tokenise()\n",
    "        self.get_lowergram_counts()\n",
    "        self.get_ngram_counts()\n",
    "        \n",
    "    def tokenise(self):\n",
    "        self.input_list = re.split(r'\\s|[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?]',self.text)\n",
    "        self.input_list = filter(lambda tok: tok.strip(),self.input_list)\n",
    "        \n",
    "    def get_lowergram_counts(self):\n",
    "        ngrams = list(zip(*[self.input_list[i:] for i in range(self.n-1)]))\n",
    "        for b in ngrams:\n",
    "            if b not in self.lowergram_counts:\n",
    "                self.lowergram_counts[b] = 1\n",
    "            else:\n",
    "                self.lowergram_counts[b] += 1\n",
    "        \n",
    "    def get_ngram_counts(self):\n",
    "        ngrams =  list(zip(*[self.input_list[i:] for i in range(self.n)]))\n",
    "        for b in ngrams:\n",
    "            w1 = tuple([x.lower() for x in b[:-1]])\n",
    "            w2 = b[-1].lower()\n",
    "            if w1 not in self.ngram_counts:\n",
    "                self.ngram_counts[w1] = {}\n",
    "            if w2 not in self.ngram_counts[w1]:\n",
    "                self.ngram_counts[w1][w2] = 1\n",
    "            else:\n",
    "                self.ngram_counts[w1][w2] += 1\n",
    "\n",
    "    def get_followee_count(self,w):\n",
    "        retval = 0\n",
    "        for word in self.ngram_counts:\n",
    "            if w in self.ngram_counts[word]:\n",
    "                retval += 1\n",
    "        return retval\n",
    "\n",
    "    def get_unique_ngrams(self):\n",
    "        return sum([len(self.ngram_counts[key]) for key in self.ngram_counts])\n",
    "\n",
    "    def p_kn_lower(self,w):\n",
    "        return self.get_followee_count(w)/self.get_unique_ngrams()\n",
    "\n",
    "    def p_kn(self,ngram):\n",
    "        global DELTA\n",
    "        w1 = ngram[0]\n",
    "        w2 = ngram[1]\n",
    "        prob = 0\n",
    "        if w1 in self.ngram_counts and w2 in self.ngram_counts[w1]:\n",
    "            prob = max(self.ngram_counts[w1][w2] - DELTA, 0)/self.lowergram_counts[w1]\n",
    "        prob += (DELTA/self.lowergram_counts[w1]) * len(self.ngram_counts[w1]) * self.p_kn_lower(w2)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_sent = \"Et tout ceci dans le respect des principes que nous avons toujours soutenus\"\n",
    "source_words = w.tokenize(source_sent)\n",
    "target_sent = \"\"\n",
    "for word in source_words:\n",
    "    if word in model_source.wv.vocab:\n",
    "        target_Sent += str(top_translations_list(pairs.loc[df['francais'] == word,'francais'].iloc[1]))\n",
    "    else:\n",
    "        target_sent += word\n",
    "content = [x.strip() for x in english_sentences]\n",
    "kn = KneserNey(content,3)\n",
    "probs_kn = {}\n",
    "for trigram in trigrams.keys():\n",
    "    probs_kn[trigram] = kn.p_kn((trigram[0],trigram[1]))\n",
    "target_first_word = top_translations_list(pairs.loc[df['francais'] == source_words[0],'francais'].iloc[1])\n",
    "list_words = []\n",
    "for word in source_words:\n",
    "    sorted_probs_kn = sorted(probs_kn.items(),key= lambda x: x[1], reverse=True )\n",
    "    list_words.append(sorted_probs_kn[0][0])\n",
    "print list_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
